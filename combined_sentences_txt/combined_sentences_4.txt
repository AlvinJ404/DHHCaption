obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work.
And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is. This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments.
So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.
You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology.
So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined. So you will hear people throw around all sorts of terms, cryptocurrency, blockchain, consensus. And these words are kind of have floating evolving meanings right now.
Part of that is because Bitcoin, the first cryptocurrency, didn't come from academia as far as we know. It came from a community of enthusiasts on the internet. And so it doesn't necessarily have the same basis and rigor that we might expect for most of our academic fields of study. It's totally OK. We're figuring it out as we go along.
And academia is really embracing this topic. So if any of you are graduate students who are looking for an area in which to do research, I think basically the number of papers published on cryptocurrencies and blockchain technology in respected academic venues is doubling every year. So there's huge opportunity here. So cryptocurrencies are not regular currencies.
They're not a dollar or a pound or a euro, what we normally think of as currency. There's something different. Bitcoin was sort of created out of nowhere. And what does it mean to create a crypto?
And I'm going to start by showing you this picture because I want to highlight how remarkable is our ability to recognize emotions, our human ability to recognize emotions. So when you look at this image, for instance, we don't know anything about these people, but we can capture a lot of things about the atmosphere, right? We see like a positive atmosphere and these people seem to be engaged and seem to be connecting on something. All of that is emotional information that we can capture just with one image.
We have this remarkable capacity for recognizing emotions. Why is this capacity useful for us? It's very important in our social interactions. When we interact with people, we are constantly making guesses about how others feel in order to adapt the way we communicate to others' emotions.
It's very important to detect people's needs as well, and also to predict people's reactions. And actually emotions are so important in our lives and have a strong influence in a lot of our cognitive processes. So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives.
We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?
Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body.
So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures. We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance.
So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.
I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras?
Any idea? What's the most popular research from a computer vision perspective to recognize emotions? So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now.
And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.
This software works very well, so it's very well at detecting the patterns, it's very while detecting the key points, it's complicated to associate the correct emotion to an expression, a phase expression, but it works pretty well in some applications. It's being used for test users, for instance, but the problem of this software is that it does not work in the wild. So we would like to have machines that can recognize emotions in any condition. So, if we go back to this image that I showed you before, what happens when we run this type of software here?
So if we run this type of commercial software, we can detect very well the face of this woman, we can detect the key points, and the software is answering in this case that this woman is feeling happiness because it detects a smile with a strong confidence, it detects attention. I don't know exactly how this detection of attention works here, but actually it's pretty accurate. But what happens with him? So what happens with him is that the system is not working.
So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.
More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise.
The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise. But when we look at the context, we realize that it's not surprise, right? So the thing is that this kit, we see a facial expression here but has nothing to do with the emotion, it's related to the action he is performing.
So some of our facial expressions have nothing to do with our emotions but with other things. For instance, when you talk, you change a lot your facial expression. It has nothing to do with your emotion. And another difficulty is giving the correct emotional meaning to an isolated facial expression.
So I'm going to show you some examples of experiments that were performed by psychologists that were studying what's the level of agreement in giving an emotional label to a specific facial expression, like here. So here we see clearly a facial expression, but if I ask you, what's this facial expression communicating? Well, you might think maybe anger, maybe contempt, this gas fear. So usually people agree that this is a negative, valence emotion, but it's not clear what category.
The thing is that if I give you some context suddenly, we probably will agree that this is disgust and this is what happens. People agree here that the face is disgust. But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts.
And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.
So maybe with the expression, you can say something about basic emotions, but there are some secondary emotions or social emotions that are very interesting that if you don't see the context, you cannot say anything about them. Like here, once you see the context, you might say that this person is feeling confident. When you define confidence as feeling of being certain, conviction that an outcome will be favorable, encouraged, or proud. So that's the idea of the emotion recognition in context project.
So what we want to do is we want to go from images like this one, so not just relating on the face and looking also at the context of the person, trying to recognize emotions, and for that, so this is a computer vision project. We know that in computer vision nowadays what is working best is deep learning models. So our idea was, okay, let's try to model this problem using deep learning. So first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any data set where we could see people in their context.
So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public data sets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.
So where we asked annotators to label what emotion category they thought this person was expressing in this specific situation. We had two different interfaces. This is the one of emotion categories. And we have another one for continuous dimensions.
So there are different ways of representing emotions in a machine. Categories is one of the most common and emotional dimensions is the other one. Maybe it's less popular. But the idea of the continuous dimensions is to label according to valence, which is is a dimension that just measures whether someone is feeling something positive or something negative.
A rousal is measuring whether someone is in calm or very ready to act, very agitated. And then dominance that is measuring whether someone is feeling dominated by the situation or the opposite is that someone is feeling in control of the situation. And then we also collected some demographics of the person in the picture, like the gender or the estimated age. And we use cloud sourcing for collecting all of these annotations.
In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem.
So what we did, it's a very simple model. This is the representation of the architecture. So basically, we have as input the image, and we know the location of the target person that we are trying to recognize the emotion of. And then we have one module that is extracting person features.
So it's fully convolutional and we extract features of the bounding box containing the person. And then we have another module which is the context features that takes as an input the whole image and extract also enough using a fully convolutional network features about the context. And then we merge these features. We have one fully connected layer.
And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get.
This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture. Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation.
Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.
So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review.
review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner. Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not.
But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.
For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text.
And this is one of the state of the art models. It's called deep moji. It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines.
And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.
And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo.
So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text. And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer.
So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis."
