original_sentence,summary,level,FRE
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","A pseudonym online shared a paper and code for Bitcoin, calling it electronic cash, inviting others to join the peer-to-peer network.",9,40.4914285714286
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.",Bitcoin was introduced 10 years ago by an unknown person who shared a paper and code on a forum.,8,71.78157894736843
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","Around 10 years ago, Bitcoin appeared with a paper and code posted by a pseudonymous person online.",7,50.2388235294118
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.",Bitcoin was introduced by someone using a pseudonym 10 years ago; they shared a paper and code on a forum.,6,68.09500000000004
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","Someone using a pseudonym introduced Bitcoin 10 years ago by sharing a paper and code, calling it electronic cash.",5,49.518421052631595
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","About 10 years ago, a pseudonym introduced Bitcoin, describing it as electronic cash and sharing code in a forum.",4,45.06578947368425
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.",The concept of Bitcoin was introduced 10 years ago by an anonymous individual who posted code and a paper in a forum.,3,46.06863636363636
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","Ten years ago, a person hidden behind a pseudonym shared the idea of Bitcoin on an email list, describing it as electronic cash and provided code to be a part of the network.",2,42.59454545454548
"obvious is what is a cryptocurrency? So this word was kind of invented 10 years ago when, I don't know how many of you know the origin story of where Bitcoin came from, but basically a pseudonym on the internet dropped a paper and some open source code in a forum on an email list and said, hey, I have this idea for this thing called Bitcoin. It's kind of like electronic cash. Here's how I think it could work. And here is some code if you want to run it and become part of this peer-to-peer network. We don't know who this person is.","A decade ago, a person using a pseudonym introduced the concept of Bitcoin through an online forum, explaining its potential as electronic cash and sharing code for participating in a peer-to-peer network. The identity of this person remains unknown.",1,33.02711538461543
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","This person has disappeared online and in the world but created impactful technology that generated billions in value and inspired problem-solving beyond payments, like banking the unbanked and improving traceability. This class, however, focuses on technology, not applications.",1,9.4447368421053
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","The person has vanished from both the internet and the world but has innovated technology resulting in billions of dollars and new problem-solving ideas. This class will focus on technology, not its applications.",2,36.26931818181819
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","The person disappeared from the internet and the world but created valuable technology that inspired widely. This class will cover technology, not applications.",3,25.962500000000034
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","This innovator disappeared online but created technology worth billions, inspiring new solutions beyond payments. The class will focus on technology and infrastructure.",4,11.088181818181852
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","This person disappeared online but created technology worth billions, inspiring solutions beyond payments. This class focuses on technology, not applications.",5,6.335000000000008
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","The person vanished online but created billion-dollar technology inspiring new solutions. This class covers technology, not applications.",6,-0.8513235294117578
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","The person vanished but created billion-dollar technology inspiring new solutions. This class covers technology, not applications.",7,-2.2099999999999795
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","They vanished but created valuable technology inspiring new solutions. This class covers technology, not applications.",8,-3.8174999999999955
"This person is basically virtually disappeared from the internet and from the world. But it's created something that has captured so many people's imaginations and has sort of, depending on how you measure it, created billions and billions of dollars of economic value and sort of inspired a lot of people to think about how to use this technology to solve a myriad of different problems, not just electronic payments. So cryptocurrencies and the technology behind them are inspiring people to think about how to bank the unbanked, add more auditability and traceability to our world, get rid of trusted intermediaries and institutions in certain situations, and basically solve every problem if you read about what blockchains can do on the internet. Now, that's not exactly what this class is about. This class is not going to be about applications. This class is going to be about technology and infrastructure.","They disappeared but created technology worth billions inspiring solutions. This class covers tech, not apps.",9,35.662499999999994
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","This is a guide on creating cryptocurrencies, the key components and techniques, and their applications. However, digital identity and health care records are not covered. The focus is on the technology and the difference between cryptocurrencies and regular currencies. Terms in this space are evolving.",9,37.49625000000003
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","You'll learn the process of creating a cryptocurrency, its key parts, essential techniques, and applications. Digital identity and health records are excluded. We'll discuss technology and differences with regular currencies. The terms are still evolving.",8,23.919464285714326
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","You'll learn how to create a cryptocurrency, its key parts, important techniques, and possible applications. We will not cover digital identity or health records, just the technology and differences from regular currencies. Terms are still evolving.",7,30.155
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","You will learn about creating a cryptocurrency, its components, and techniques, and how to apply it. Digital identity and health records are not included. We will discuss technology and differences from regular currencies. Terms in this area are still developing.",6,35.94500000000002
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","You will be taught how to make a cryptocurrency, its components, techniques, and applications. Digital identity and health care records are excluded. The focus will be on technology and differences from regular currencies. Terms are currently evolving.",5,35.10570945945949
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","Learn the creation, components, and techniques of a cryptocurrency, including its applications. Topics like digital identity and health records are excluded. We'll focus on technology differences from regular currencies. The terms are still being established.",4,21.502321428571435
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","You will learn to create a cryptocurrency, understand its parts, essential techniques, and applications. Excluded are topics like digital identity and health records. Focus will be the technology and differences from regular currencies. Terms are still defined.",3,32.819222972972995
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","In this lesson, you will learn about creating a cryptocurrency, its important parts, the techniques involved, and potential applications. We will not cover digital identity or health care records, focusing instead on the technology itself and its differences from regular currencies. The terms in this space are still being formalized.",2,32.562333333333356
"You are going to learn how to create a cryptocurrency, what goes inside a cryptocurrency, what's important, what are the techniques, and what application you choose to apply that to down the line. That's kind of up to you. But we're not going to be doing digital identity or health care records or something like that. We're going to be talking about the technology. So a big question is how are cryptocurrencies different from regular currencies? And another thing that I want to make really clear is that the terms in this space are still being defined.","This guide will teach how to create a cryptocurrency, covering its components, crucial techniques, and possible applications. We will not involve digital identity or health care records, instead focusing on the core technology and its distinctions from traditional currencies. The terminology in this field is still being clarified.",1,28.444999999999993
"So you will hear people throw around all sorts of terms, cryptocurrency, blockchain, consensus.",You will hear terms like cryptocurrency and blockchain frequently.,9,47.30000000000004
And these words are kind of have floating evolving meanings right now.,These terms currently have evolving meanings.,9,45.64500000000001
"Part of that is because Bitcoin, the first cryptocurrency, didn't come from academia as far as we know.","Bitcoin, the first cryptocurrency, didn't come from academia.",7,18.940000000000026
It came from a community of enthusiasts on the internet.,It originated from online enthusiasts.,7,-1.2799999999999727
And so it doesn't necessarily have the same basis and rigor that we might expect for most of our academic fields of study.,"Thus, it lacks the standard academic rigor.",6,54.70142857142861
It's totally OK. We're figuring it out as we go along.,It's fine; we are learning as we go.,8,92.965
"So you will hear people throw around all sorts of terms, cryptocurrency, blockchain, consensus.",You will hear terms like cryptocurrency frequently.,8,42.61571428571432
And these words are kind of have floating evolving meanings right now.,These terms have changing meanings currently.,6,73.84500000000001
"Part of that is because Bitcoin, the first cryptocurrency, didn't come from academia as far as we know.",Bitcoin wasn't created in academia but by enthusiasts.,5,18.940000000000026
It came from a community of enthusiasts on the internet.,It was developed by internet enthusiasts.,6,31.545000000000016
And academia is really embracing this topic.,Academia widely accepts this topic.,9,15.640000000000015
"So if any of you are graduate students who are looking for an area in which to do research, I think basically the number of papers published on cryptocurrencies and blockchain technology in respected academic venues is doubling every year.",Graduate students seeking research topics should note that publications on cryptocurrencies and blockchain technology double yearly.,6,5.532499999999999
So there's huge opportunity here.,There is a large opportunity here.,8,59.745000000000005
So cryptocurrencies are not regular currencies.,Cryptocurrencies differ from regular currencies.,7,-35.11999999999995
"They're not a dollar or a pound or a euro, what we normally think of as currency.","They are not like dollars, pounds, or euros.",8,92.965
"So if any of you are graduate students who are looking for an area in which to do research, I think basically the number of papers published on cryptocurrencies and blockchain technology in respected academic venues is doubling every year.",Research areas for graduate students include rapid growth in blockchain and cryptocurrency papers.,5,24.440000000000026
And academia is really embracing this topic.,The academic world embraces this topic widely.,7,30.53000000000003
There's something different.,They are something different.,9,54.72500000000002
There's huge opportunity here.,There is significant opportunity available.,6,-52.039999999999964
So there's huge opportunity here.,"Thus, there are major opportunities.",5,32.56000000000003
Bitcoin was sort of created out of nowhere.,Bitcoin was seemingly created from nothing.,1,45.64500000000001
And what does it mean to create a crypto?,What does creating a cryptocurrency mean?,2,31.545000000000016
"And I'm going to start by showing you this picture because I want to highlight how remarkable is our ability to recognize emotions, our human ability to recognize emotions.",I'll begin by showing you a picture to highlight our remarkable ability to recognize emotions.,3,22.410000000000025
"So when you look at this image, for instance, we don't know anything about these people, but we can capture a lot of things about the atmosphere, right?","When we see this image, we may not know the people, but we can sense the atmosphere.",4,85.07411764705884
We see like a positive atmosphere and these people seem to be engaged and seem to be connecting on something.,We see a positive atmosphere and people engaged and connecting.,5,44.405
All of that is emotional information that we can capture just with one image.,All this emotional info is captured with one image.,6,56.70000000000002
Bitcoin was sort of created out of nowhere.,Bitcoin was created from nothing.,7,49.48000000000002
And what does it mean to create a crypto?,What does it mean to create crypto?,8,90.95857142857145
I'll begin by showing you a picture to highlight our remarkable ability to recognize emotions.,I'll show you a picture to highlight our emotion-recognition ability.,9,10.564999999999998
All of that is emotional information that we can capture just with one image.,That's emotional information we get from one image.,1,29.515000000000015
We have this remarkable capacity for recognizing emotions.,We can recognize emotions well.,9,49.48000000000002
Why is this capacity useful for us?,Why is this skill useful?,8,100.24000000000002
It's very important in our social interactions.,It matters a lot in our social life.,7,92.965
"When we interact with people, we are constantly making guesses about how others feel in order to adapt the way we communicate to others' emotions.",We guess others' feelings to adapt our communication when interacting with people.,6,32.504999999999995
"It's very important to detect people's needs as well, and also to predict people's reactions.",Detecting people's needs and predicting reactions are vital.,5,29.515000000000015
And actually emotions are so important in our lives and have a strong influence in a lot of our cognitive processes.,Emotions greatly impact our lives and cognitive functions.,4,40.09
We have this remarkable capacity for recognizing emotions.,Recognizing emotions is an outstanding ability we possess.,3,-2.2099999999999795
Why is this capacity useful for us?,Understanding this emotion-recognition ability’s usefulness is essential.,2,-90.3271428571428
It's very important in our social interactions.,"In social interactions, its importance is indisputable.",2,-5.727142857142809
"It's very important to detect people's needs as well, and also to predict people's reactions.",It is crucial to perceive others’ needs and predict their reactions.,1,72.61545454545457
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Our attention, memory, and learning are affected by emotions. AI of the future is envisioned to have emotional intelligence, seen in many movies. Emotions are complex.",9,35.346025641025676
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions impact attention, memory, and learning. AI is predicted to have emotional intelligence, as often shown in movies. Emotions remain complex.",8,26.50142857142862
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions play a crucial role in attention, memory, and learning. Future AI with emotional intelligence is often depicted in movies. Emotions are complex.",7,33.53159420289856
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions affect attention, memory, and learning processes. Future AI is expected to have emotions, as shown in movies. Emotions are complicated.",6,34.55857142857147
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions modulate attention and affect memory and learning. Future AI is envisioned to have emotional intelligence, as seen in movies. Emotions are complex.",5,26.175072463768117
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions influence memory, attention, and learning. Future AI may have emotional intelligence, as seen in movies. Emotions are intricate.",4,22.301403508771955
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions affect attention mechanisms, memory, and learning. Future AI, often shown in movies, will have emotional intelligence. Emotions are complex.",3,22.40833333333333
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions modulate our attention, influencing memory and learning. AI of the future, shown in movies, will have emotional intelligence. Emotions are complex.",2,30.19166666666669
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions modulate our attention mechanisms and influence memory and learning. Future AI, shown in many movies, is expected to have emotional intelligence. Meanwhile, emotions are very complex.",1,25.366666666666674
"So for instance, they modulate our attention mechanisms and they strongly influence our memory and also our learning process. So emotions are so important in our lives. We imagine the AI of the future, so I'm going to ask how many of you have seen at least one of these movies here? So most of you have seen at least one of these movies. So you know what I said, I'm not going to make any spoiler, but you know what I'm saying when I say that the machines that we imagine in the future, all of them have some kind of emotional intelligence. But on the other side, emotions are so complex, right?","Emotions modulate attention and influence both memory and learning processes. AI of the future, often depicted in films, is imagined to possess emotional intelligence. Emotions are highly complex.",1,19.097380952380973
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's not easy to understand others' feelings. We can know some emotional states, but it's often difficult to understand both others and ourselves. How can machines handle this? Emotions change many things in our body. For example, nervousness raises heart rate and respiration, and changes in sweating, facial expressions, gaze, pupils, blood pressure, and posture occur.",1,50.4384285714286
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","Even people find understanding feelings hard. We get some emotional clues but often fail to know how others or we feel. So, how can machines grasp this? Emotions trigger many bodily changes. For example, nervousness alters heart rate, breathing, sweating, facial expressions, gaze, pupils, blood pressure, and posture.",2,56.09100000000001
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","Understanding how others feel is hard. We get some clues about emotions but often can't tell feelings. How can machines do it? Emotions change many body aspects, like heart rate, breathing, sweating, facial expressions, gaze, pupils, blood pressure, and posture.",3,61.32500000000002
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It’s tough to understand feelings. We sense some emotions, but often we can't know how we feel. How can machines do this? Emotions change many body processes, like heart rate, breathing, sweating, facial expressions, gaze, pupils, blood pressure, and posture.",4,65.555
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's hard to understand how others feel. We can get some emotional clues, but it's complex. How can machines handle this? When emotions change, our body shows many signs, like heart rate, breathing, sweating, facial expressions, gaze, pupils, blood pressure, and posture.",5,65.24892857142859
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's often hard for us to understand emotions. We can get some emotional information, but it's complex. How can machines do this? When emotions change, our body shows many signs like heart and respiratory rate, sweating, facial expressions, gaze, pupils, blood pressure, and posture.",6,53.388181818181835
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's hard for us to know others' feelings. We can get some emotional hints, but it's tricky. How can machines manage this? Emotional changes affect our body: heart rate, breathing, sweating, facial expressions, gaze, pupils, blood pressure, and posture all change.",7,62.30929878048781
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's tough to understand how others feel. Some emotional clues, but it's complex. How do machines do it? Emotions change body signals: heart, breathing, sweat, facial expressions, gaze, pupils, blood pressure, and posture all shift.",8,62.59375000000003
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","It's hard to know feelings. Emotional clues exist, but it's tricky. How do machines manage it? Emotions change body signals: heart, breathing, sweat, facial expressions, gaze, pupils, blood pressure, and posture shift.",9,58.596250000000026
"Even for us sometimes it's difficult to know how others feel. We can grasp some information about the upper and emotional states of people, but it's complex sometimes to know how others feel or even how we feel. So how can machines do something that complex? Well, the thing is that when our emotions change, there are a lot of things that change in our body. So for instance, when we are nervous, usually our heart rate increases, also our respiration rate increases, we sweat, so the electrodermal activity in our skin also changes. Our facial expression changes, our gaze patterns change, our pupil changes as well, our blood pressure, and we have different body postures.","Knowing feelings is hard. Emotional clues are tricky. How do machines manage them? Emotions change body signals: heart, breathing, sweat, facial expressions, gaze, pupils, blood pressure, and posture shift.",1,59.44866379310349
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Gestures vary with emotions. Voice, writing, and typing patterns change. Sensors capture these signals. Labeled data helps find emotion patterns, enabling algorithms to detect emotions.",9,34.675250000000034
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Emotions alter gestures, voice, writing, and typing. Sensors capture these signals, and labeled data aids in identifying patterns for emotion detection algorithms.",8,18.77909090909094
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","We show emotions in gestures, voice, and writing. Sensors can capture these signals and labeled data helps detect emotion patterns for algorithms.",7,45.69727272727275
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Gestures, voice, and writing show how we feel. Sensors capture these signals. Labeled data finds emotion patterns, aiding detection by algorithms.",6,46.64428571428573
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Emotions change our gestures, voice, and writing. Sensors capture these signals. Labeled data helps find emotion patterns, and algorithms detect these emotions.",5,41.72803030303032
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Our gestures, voice, and writing change with emotions. These signals are captured by sensors. Labeled data is used to find patterns, enabling algorithms to detect emotions.",4,51.61525641025645
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Our emotions alter gestures, voice, and writing styles. Sensors can capture these signals, and the labeled data allows patterns to be found that help develop algorithms for emotion recognition.",3,40.42094827586209
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Various gestures, voice tones, and writing styles vary based on emotions. Sensors can capture these changes. When signals are labeled with their respective emotions, patterns can be identified, allowing development of algorithms that recognize emotions.",2,30.627619047619078
"We use different gestures depending on how we feel. Our voice can change as well, the way we write, the way we type in our phones, for instance. So there are a lot of signals that change with our emotions. So what we can do is all of these signals can be captured with different types of sensors. And then we capture these signals, and if we have data, label data with emotions, What we can do is we can try to find patterns in these signals that correlate with emotions and develop machine learning algorithms or systems that can recognize the emotions using this type of information. The same way we process other type of information.","Gestures, voice pitch, and writing habits shift with emotional state. By capturing these signals with sensors, labeled emotional data aids in discovering correlating patterns, facilitating the creation of machine learning algorithms to identify emotions.",1,7.938823529411792
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|","I will discuss capturing emotional data, starting with vision modality which is my expertise. What research exists in recognizing emotions through cameras? Any idea on popular techniques?|",1,22.233333333333377
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|","I will discuss emotional data, starting with cameras. What techniques are used to recognize emotions through cameras?|",2,38.960441176470624
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",I will discuss capturing emotions through cameras. Which research is popular for this in computer vision?|,3,45.377500000000026
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",Let's discuss capturing emotions using cameras. What popular computer vision research exists on this?|,4,24.487142857142885
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",Let's explore how cameras capture emotions. What known computer vision techniques achieve this?|,5,31.037500000000023
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",How do cameras detect emotions? Which computer vision research is popular in this field?|,6,48.65857142857146
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",How do cameras capture emotions? Which research in computer vision is most popular?|,7,44.05288461538464
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",How do cameras read emotions? What's the top research on this in computer vision?|,8,60.74428571428575
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",How do cameras read emotions? What's the leading research on this in computer vision?|,9,54.70142857142861
"I'm going to focus first on the, so I'm going to talk about some examples of how can we capture this emotional information. And I'm going to start with vision modality, which is one of my areas of expertise. So visual modality, cameras. What has been done so far for recognizing emotions from information captured by cameras? Any idea? What's the most popular research from a computer vision perspective to recognize emotions?|",How do cameras read feelings? What's the leading research on this in computer vision?|,1,60.74428571428575
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions are key to recognizing emotions. Years ago, during my PhD, this was unsolved. I worked on software for recognizing facial points under controlled conditions, but it failed. Now, after 10 years, software can accurately recognize and track facial points to match expressions with emotions. Affectiva's commercial software detects, analyzes, and correlates facial points with emotions.",9,39.42189473684212
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions are the main method to identify emotions. A decade ago, during my PhD, the problem was unsolved. I worked on recognizing facial points in controlled images, but it failed. Now, software tracks and matches facial points with emotions effectively. Affectiva's software detects points, analyzes them, and relates expressions to emotions.",8,40.09438461538463
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Recognizing emotions through facial expression is common. Ten years ago, during my PhD, this was unsolved. I worked on facial recognition software in controlled settings, but it failed. Today, software accurately tracks facial points and connects expressions to emotions. Affectiva's commercial software does this effectively.",7,39.78000000000003
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions are the main way to recognize emotions. During my PhD ten years ago, it was unsolved. I worked on software for facial points, but it failed. Now, software matches facial points to emotions well. Affectiva's software detects and analyzes facial points to relate them to emotions.",6,57.853500000000025
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Recognizing emotions through facial expressions is common. A decade ago, during my PhD, it was unsolved. I worked on identifying key facial points in controlled environments, but the software failed. Now, programs effectively match facial points to emotions. Affectiva's commercial software detects and analyzes facial points to relate them to emotions.",5,37.23494117647061
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Recognizing emotions through facial expressions is common now. A decade ago, during my PhD, it was unsolved. I worked on recognizing key facial points in controlled conditions, but it failed. Today, software effectively matches these points to emotions. Affectiva's commercial software detects and analyzes facial points to relate them to emotions.",4,40.55258823529414
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions are crucial for noticing emotions. A decade ago, during my PhD, this issue was unsolved. I worked on recognizing key facial points in controlled environments, but the software failed. Now, programs effectively match facial points to emotions. Affectiva's software detects and analyzes facial points, relating them to emotions.",3,39.32900000000001
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions help recognize emotions. Ten years ago, during my PhD, it was unsolved. I worked on recognizing facial points in controlled environments, but the software failed. Now, software effectively matches facial points to emotions. Affectiva's software detects and relates these points to emotions.",2,42.16209090909095
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","Facial expressions are used to recognize emotions. Ten years ago, during my PhD, the problem was unsolved. I worked on recognizing key facial points, but the software did not work. Now, software effectively matches facial points to emotions. Affectiva's software detects and relates points to emotions.",1,50.366565217391326
"So facial expression is actually the most common approach to recognize emotions. So when I was doing my PhD, I finished like 10 years ago now. And this was a completely unsolved problem. So I was working a little bit in this type of software, trying to recognize key points in the faces, working with images acquired under controlled environments, and nothing was working by then. So now after 10 years, we have these softwares that work very well in recognizing some of the key points on your face, tracking these key points, and doing some matching between the emotions, so the expressions that are observed and the possible emotions that they are expressing. This is an example of a commercial software from a company called Affectiva, where you see that the software detects your key points, analyzes the configuration, and makes some kind of correspondence between this expression and the possible emotion that you are expressing with the facial expression.","People use facial expressions to recognize emotions. A decade ago, this problem was not solved in my PhD. The software to identify key facial points didn't work. Now, software identifies facial points and links them to emotions effectively. Affectiva's software does this very well.",1,49.85300000000004
"This software works very well, so it's very well at detecting the patterns, it's very while detecting the key points, it's complicated to associate the correct emotion to an expression, a phase expression, but it works pretty well in some applications.","The software works well at detecting patterns and key points, but associating emotions to expressions is complicated. It works in some applications.",9,45.69727272727275
"It's being used for test users, for instance, but the problem of this software is that it does not work in the wild.",The software is used for test users but fails in real-world scenarios.,9,67.75500000000001
So we would like to have machines that can recognize emotions in any condition.,We want machines to recognize emotions in any condition.,9,37.900000000000034
"So, if we go back to this image that I showed you before, what happens when we run this type of software here?",What happens when we run this type of software on the image I've shown before?,8,78.81000000000003
"So if we run this type of commercial software, we can detect very well the face of this woman, we can detect the key points, and the software is answering in this case that this woman is feeling happiness because it detects a smile with a strong confidence, it detects attention.","Running commercial software, it detects a woman's face and key points. It claims she is happy due to her smile and detects attention confidently.",4,60.70500000000001
"I don't know exactly how this detection of attention works here, but actually it's pretty accurate.","The detection of attention is unclear, but it is accurate.",9,52.86500000000001
"This software works very well, so it's very well at detecting the patterns, it's very while detecting the key points, it's complicated to associate the correct emotion to an expression, a phase expression, but it works pretty well in some applications.",The software efficiently detects patterns and key points but struggles with emotions in expressions.,5,41.553571428571445
"It's being used for test users, for instance, but the problem of this software is that it does not work in the wild.","The software is used by test users, but it fails in natural conditions.",5,69.99384615384618
So we would like to have machines that can recognize emotions in any condition.,We want machines that recognize emotions in any situation.,7,28.50000000000003
"So, if we go back to this image that I showed you before, what happens when we run this type of software here?","Refer to the image shown earlier, what results from running this software?",5,67.75500000000001
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|","The system isn't functioning because key facial points aren't visible. Glasses cause strong occlusions and highlights, plus we have a profile view. The software struggles with nonfrontal faces and partial occlusions in open environments.|",1,46.037549019607866
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|","The system isn't working due to occlusions from glasses and profile view, making key facial points invisible. Nonfrontal faces and occlusions cause failure in open environments.|",2,37.455384615384645
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|","Occlusions and profile view hinder the system, making key facial points invisible. Software struggles with nonfrontal faces in open settings.|",3,40.17500000000001
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|",Glasses and profile views cause system issues by hiding facial points. Software fails with nonfrontal faces in open areas.|,4,59.16092105263158
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|",The system fails when glasses and nonfrontal views hide facial points. It doesn't work well with occlusions in open areas.|,5,69.78500000000001
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|","Glasses and profile views hide facial points, causing system failure.|",6,61.32500000000002
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|",System fails due to glasses and profile views.|,7,82.39000000000001
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|",Glasses and profile views cause system errors.|,8,66.7871428571429
"But what happens with him? So what happens with him is that the system is not working. So here we have a problem and it's all the key points of the face are not actually visible. So we have strong occlusions and highlights because of the glasses of the person. And also we have a profile view, so it's not a frontal view anymore where everything is visible, right? So this type of software, when you run them in open environments, you face that when you have nonfrontal faces and partial occlusions, they might not work as well as we would like.|",System fails.|,9,77.90500000000002
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|","Another problem arises with a face example. Running the software shows surprise with an open mouth, detecting surprise perfectly.",1,41.350394736842105
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",Another example of problems: the software detects a face's surprise with perfect accuracy.,2,24.440000000000026
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|","There are more problems, like this face example. The software detects an open mouth indicating surprise correctly.",3,58.8663235294118
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",Another example shows a face. The software identifies the surprise shown by an open mouth accurately.,4,50.66500000000002
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",We meet another problem: a face example. The software perfectly detects the surprise with the mouth open.,5,58.8663235294118
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",Here's another example: a face. The software sees surprise and detects the open mouth perfectly.,6,52.58250000000001
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|","One more example: a face. The software detects surprise accurately, observing an open mouth.",7,54.70142857142861
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",Here's another example where the software detects a face's surprise expression perfectly.,8,18.405
"More problems. So here we have another example. Here is a face. If we run the software, it says, you see surprise. The mouth is open. So it's detecting perfectly well a very prototypical expression of surprise.|",The software detects surprise in a face accurately. The mouth is open.,9,73.84500000000001
"But when we look at the context, we realize that it's not surprise, right?",Examining context shows it's not surprising.,9,17.44500000000002
"So the thing is that this kit, we see a facial expression here but has nothing to do with the emotion, it's related to the action he is performing.","This face shows the action, not emotion.",7,78.87285714285717
So some of our facial expressions have nothing to do with our emotions but with other things.,Some expressions aren't linked to emotions.,6,45.64500000000001
"For instance, when you talk, you change a lot your facial expression.",Talking changes your facial expressions.,8,32.56000000000003
It has nothing to do with your emotion.,It’s not about emotion.,9,54.72500000000002
And another difficulty is giving the correct emotional meaning to an isolated facial expression.,It’s hard to read an isolated face.,5,78.87285714285717
"ut when we look at the context, we realize that it's not surprise, right?",Context shows it's not surprising.,7,49.48000000000002
"o the thing is that this kit, we see a facial expression here but has nothing to do with the emotion, it's related to the action he is performing.","The face shows action, not emotion.",8,73.84500000000001
o some of our facial expressions have nothing to do with our emotions but with other things.,"Some faces show actions, not feeling.",4,73.84500000000001
"or instance, when you talk, you change a lot your facial expression.",Talking changes your facial movements.,7,32.56000000000003
"So I'm going to show you some examples of experiments that were performed by psychologists that were studying what's the level of agreement in giving an emotional label to a specific facial expression, like here.",Let me show you some experiments by psychologists studying agreement on labeling emotions to facial expressions.,9,21.39500000000001
"So here we see clearly a facial expression, but if I ask you, what's this facial expression communicating? Well, you might think maybe anger, maybe contempt, this gas fear.","We see a facial expression; you might guess it indicates anger, contempt, or fear.",8,59.68214285714288
"So usually people agree that this is a negative, valence emotion, but it's not clear what category.","People agree this expression shows a negative emotion, but the category is unclear.",7,30.947692307692336
"The thing is that if I give you some context suddenly, we probably will agree that this is disgust and this is what happens.","Given context, we likely agree the face shows disgust.",6,66.10000000000002
People agree here that the face is disgust.,Everyone agrees the face shows disgust.,9,59.745000000000005
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","In different contexts, the same facial expression can indicate anger. Examples and experiments show context influences emotion perception. This project started in Barcelona and continues with MIT. Machines must consider the context to recognize emotions accurately. Collaborators study scene context to better understand emotions.",9,19.089363636363657
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","The emotion perceived can change with context, even if the facial expression is the same. This inspired a project in Barcelona, now continuing with MIT, to create machines that look at the context to read emotions. They especially focus on the scene context.",8,58.5006201550388
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","Same facial expression means anger in another context. Researchers use examples and experiments showing context determines emotion perception. This motivated a project from Barcelona to MIT, aiming to make machines recognize emotions by understanding context, especially scene context.",7,11.420438596491266
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","The way emotions are perceived changes with context. Experimenting with the same facial expression in different situations shows this. This inspired a project, now involving MIT, aiming to develop machines that recognize emotions by also considering context.",6,34.26261261261263
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","Experiments show that context affects how we perceive the same facial expressions. This motivated a project that started in Barcelona and now involves MIT. The goal is to make machines that understand emotional context, focusing on scene context.",5,44.815175438596526
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","Research shows context can change our perception of the same facial expression. This led to a project involving MIT, aiming to develop machines that understand emotions through context.",4,50.61785714285716
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","Context changes emotion perception, as shown through experiments. This inspired a project, now involving MIT, to help machines read emotions by considering context.",3,36.997282608695684
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.","Context affects how we see emotions in faces. This started a project with MIT to help machines read emotions by looking at context, especially scene context.",2,56.97846153846157
"But if you give you a different context, then people strongly agree that this is anger. So they have a lot of examples and they run a lot of experiments in using the exact same facial expression in different contexts. And they realize that the context strongly influences the way we perceive emotions. So this is the motivation of one of the projects I'm working on that started here in Barcelona and now I'm still working on it in collaboration with the effective computing group at MIT media lab and the idea behind this project is okay, we know there's a lot of information in the face, that's true, but for developing machines that accurately recognize emotions, we can just rely on the face. We need to understand the context of the person and in particularly we are working in the scene context, in the situation of the person. And the interesting thing of working in this type of context is that suddenly you can say much more about the emotion.",Context changes how we see emotions in faces. A project with MIT was created to help machines understand this. Focus is on scene context.,1,68.28999999999999
"So maybe with the expression, you can say something about basic emotions, but there are some secondary emotions or social emotions that are very interesting that if you don't see the context, you cannot say anything about them.","Facial expressions can show basic emotions, but for complex emotions, context is essential.",1,30.947692307692336
"Like here, once you see the context, you might say that this person is feeling confident.",Context reveals that this person is feeling confident.,2,50.66500000000002
"When you define confidence as feeling of being certain, conviction that an outcome will be favorable, encouraged, or proud.",Confidence is the feeling of certainty and belief in a favorable outcome.,3,39.55500000000001
So that's the idea of the emotion recognition in context project.,This is the aim of the emotion recognition in context project.,4,57.23363636363638
"So what we want to do is we want to go from images like this one, so not just relating on the face and looking also at the context of the person, trying to recognize emotions, and for that, so this is a computer vision project.",We aim to recognize emotions from facial images and context using a computer vision project.,5,39.33000000000001
We know that in computer vision nowadays what is working best is deep learning models.,Deep learning models are currently the best in computer vision.,6,52.86500000000001
"So maybe with the expression, you can say something about basic emotions, but there are some secondary emotions or social emotions that are very interesting that if you don't see the context, you cannot say anything about them.","Expressions show basic emotions, but without context, complex emotions are hard to identify.",7,24.440000000000026
"Like here, once you see the context, you might say that this person is feeling confident.",Context shows this person is confident.,8,59.745000000000005
"When you define confidence as feeling of being certain, conviction that an outcome will be favorable, encouraged, or proud.",Confidence means feeling certain and believing in a good outcome.,9,52.86500000000001
So that's the idea of the emotion recognition in context project.,This is the emotion recognition in context project.,1,40.09
"So our idea was, okay, let's try to model this problem using deep learning. So first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any data set where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public data sets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We aimed to model this problem using deep learning, but the initial challenge was the lack of training data, as emotion recognition research mostly focused on faces. We collected images from search engines and public datasets, showing people in various contexts, and created annotation interfaces.",1,24.19750000000002
"So our idea was, okay, let's try to model this problem using deep learning. So first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any data set where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public data sets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We decided to use deep learning to model the problem but faced the challenge of no training data. Emotion recognition research typically focused on faces, so we gathered images from search engines and public datasets and developed annotation interfaces.",2,24.35019230769234
"So our idea was, okay, let's try to model this problem using deep learning. So first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any data set where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public data sets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We tried modeling the problem with deep learning but had no training data. Emotion recognition usually uses faces, so we collected images from search engines and datasets and made annotation interfaces.",3,27.360564516129045
"So our idea was, okay, let's try to model this problem using deep learning. So first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any data set where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public data sets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We aimed to model the problem with deep learning but lacked training data. Emotion research focused on faces, so we gathered images from search engines and datasets and created annotation tools.",4,38.27669354838713
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.",We used deep learning to model the problem but lacked training data. Emotion recognition mainly focused on faces. We collected images from search engines and datasets and made annotation tools.,5,41.58500000000001
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.",We tried to model the problem using deep learning but lacked training data. Emotion research focused on faces. We collected images from search engines and datasets and created annotation interfaces.,6,35.94500000000002
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We aimed to model the issue with deep learning but had no training data. Emotion recognition typically focused on faces, so we collected context-rich images from search engines and datasets and created annotation interfaces.",7,22.868235294117653
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","We used deep learning for this problem but lacked training data. Emotion research focused on faces, so we collected context-rich images from search engines and datasets and created annotation tools.",8,36.51000000000002
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.","Using deep learning, we modeled the issue but lacked data. We collected context-rich images from search engines and datasets and developed annotation interfaces.",9,22.2842391304348
"So our idea was, okay, let's try to model this problem using deep learning. So the first challenge that we faced, we didn't have any training data, of course, because as we said before, usually all the research on emotion recognition from images was focused on the face, so we didn't have any datasets where we could see people in their context. So we were collecting a lot of images. Some of them were manually downloaded from search engines like Google, and other images came from public datasets that were already labeled with the bounding box of the person. And the images we collected are images like the ones that you see here, so people doing different things in very different situations. And what we did is we created annotation interfaces like this one that I'm showing you here.",We used deep learning to model the issue but didn't have data. We collected context-rich images from online search and public datasets and made annotation tools.,1,50.470769230769264
So where we asked annotators to label what emotion category they thought this person was expressing in this specific situation.,We asked annotators to label the emotion category for this situation.,1,18.77909090909094
We had two different interfaces.,We used two interfaces.,9,54.72500000000002
This is the one of emotion categories.,This one is for emotion categories.,7,45.64500000000001
And we have another one for continuous dimensions.,We have another for continuous dimensions.,7,17.44500000000002
So there are different ways of representing emotions in a machine.,Machines can represent emotions in different ways.,6,30.53000000000003
Categories is one of the most common and emotional dimensions is the other one.,Categories and dimensions are common emotion representations.,5,-29.898571428571415
So where we asked annotators to label what emotion category they thought this person was expressing in this specific situation.,Annotators were asked to label emotions for this situation.,4,28.50000000000003
We had two different interfaces.,We had two different interfaces for labeling.,2,30.53000000000003
This is the one of emotion categories.,This is the interface for emotion categories.,3,30.53000000000003
And we have another one for continuous dimensions.,We have another interface for continuous dimensions.,5,6.35857142857148
Maybe it's less popular.,It might be less known.,9,117.16000000000003
"But the idea of the continuous dimensions is to label according to valence, which is is a dimension that just measures whether someone is feeling something positive or something negative.","The continuous dimensions label emotions by valence, measuring if feelings are positive or negative.",4,11.339285714285722
"A rousal is measuring whether someone is in calm or very ready to act, very agitated.",Arousal measures if someone is calm or very agitated.,6,28.50000000000003
And then dominance that is measuring whether someone is feeling dominated by the situation or the opposite is that someone is feeling in control of the situation.,Dominance measures if someone feels dominated or in control.,5,28.50000000000003
"And then we also collected some demographics of the person in the picture, like the gender or the estimated age.","We also collected the person's demographics, like gender and age.",5,35.94500000000002
"Maybe it's less popular. But the idea of the continuous dimensions is to label according to valence, which is is a dimension that just measures whether someone is feeling something positive or something negative. A rousal is measuring whether someone is in calm or very ready to act, very agitated. And then dominance that is measuring whether someone is feeling dominated by the situation or the opposite is that someone is feeling in control of the situation. And then we also collected some demographics of the person in the picture, like the gender or the estimated age. And we use cloud sourcing for collecting all of these annotations.","The idea of continuous dimensions labels emotions by valence (positive or negative), arousal (calm or agitated), and dominance (dominated or in control). Collected demographics include gender and age, using crowdsourcing for annotations.",4,5.532499999999999
Maybe it's less popular.,Maybe it's less popular.,9,33.57500000000002
"But the idea of the continuous dimensions is to label according to valence, which is is a dimension that just measures whether someone is feeling something positive or something negative.",Valence measures whether feelings are positive or negative.,5,18.940000000000026
"A rousal is measuring whether someone is in calm or very ready to act, very agitated.",Arousal measures if someone is calm or agitated.,6,29.515000000000015
"And then we also collected some demographics of the person in the picture, like the gender or the estimated age.",We gathered demographics like gender and age.,6,42.61571428571432
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","We use Amazon Mechanical Turk to create the emoric data set with 23,000 images and 34,000 annotated people. Some images have multiple people. I’ll show a simple deep learning model we developed. This is its architecture.|",9,52.00000000000003
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","We use Amazon Mechanical Turk and created the emoric data set with 23,000 images and 34,000 people annotated. Some images have multiple people. I will show our deep learning model’s architecture.|",8,46.24989247311828
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","We used Amazon Mechanical Turk to get the emoric data set. It has 23,000 images and 34,000 annotated people with some images having multiple people. I will show our simple deep learning model's architecture.|",7,51.01401960784315
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","Using Amazon Mechanical Turk, we gathered the emoric data set, which contains 23,000 images and 34,000 people, some annotated multiple times. I will introduce our simple deep learning model.|",6,43.33818965517244
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","We use Amazon Mechanical Turk to create the emoric data set with 23,000 images and 34,000 people annotated, as some images have many annotations. I will present our simple deep learning model.|",5,45.18875
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","Using Amazon Mechanical Turk, we collected the emoric data set, containing 23,000 images and 34,000 people annotated, with some images having multiple annotations. I’ll present our simple deep learning model.|",4,30.870000000000033
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","We utilized Amazon Mechanical Turk to generate the emoric data set. This comprises 23,000 images and 34,000 people annotated, with some images containing multiple annotations. I will now demonstrate our fundamental deep learning model.|",3,23.64343137254906
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","By using Amazon Mechanical Turk, we crafted the emoric data set, which incorporates a total of 23,000 images and 34,000 people annotated, as certain images have various annotations. I am going to present to you our elementary deep learning model.|",2,30.025000000000034
"In particular, we use the platform Amazon Mechanical Turk. And after this process, we came up with what we call the emoric data set, which is a collection of these images, 23,000 annotated images, 34,000 annotated people. Because for some images, we have multiple people annotated. I'm going to show you a little bit about the deep learning model that we developed as a baseline to model this problem. So what we did, it's a very simple model. This is the representation of the architecture.|","Using Amazon Mechanical Turk, we produced the emoric data set, which consists of a collection of 23,000 images and 34,000 people annotated. For certain images, we have multiple annotations. I am about to exhibit to you our basic deep learning model.|",1,46.4608943089431
"So basically, we have as input the image, and we know the location of the target person that we are trying to recognize the emotion of.",We input the image and pinpoint the target person's location for emotion recognition.,9,24.440000000000026
And then we have one module that is extracting person features.,"Then, a module extracts features of the person.",8,61.24000000000001
So it's fully convolutional and we extract features of the bounding box containing the person.,"It is fully convolutional, extracting features from the bounding box of the person.",5,37.455384615384645
And then we have another module which is the context features that takes as an input the whole image and extract also enough using a fully convolutional network features about the context.,Another module extracts context features from the entire image using a fully convolutional network.,4,17.382142857142867
And then we merge these features.,"Then, we combine these features.",9,66.40000000000003
We have one fully connected layer.,We use a fully connected layer.,9,59.745000000000005
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We separate recognition of balance, land dominance, and emotion categories, using regression as the loss function. From our experiments, we found it the best way to model data. We get these types of results and images, recognizing anticipation, excitement, engagement, and confidence in one case. Another example shows recognition of pleasure, happiness, and affection.",1,31.765833333333376
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression to model data for recognizing balance, land dominance, and different emotions. Our experiments showed it's the best method. We get results like anticipation, excitement, engagement, and confidence in one image and pleasure, happiness, and affection in another.",2,28.331666666666678
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression to model data for recognizing balance, land dominance, and emotions. Our experiments found it most effective. Results include recognizing anticipation, excitement, engagement, and confidence, plus pleasure, happiness, and affection.",3,13.589583333333366
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We identify balance, land dominance, and emotions using regression after experiments proved it best. We recognize anticipation, excitement, engagement, confidence, pleasure, happiness, and affection in images.",4,-1.5907692307691832
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","For balance, land dominance, and emotions, we use regression, the best method from experiments. Results include recognizing anticipation, excitement, engagement, confidence, pleasure, happiness, and affection.",5,4.643499999999989
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","For recognizing balance, land dominance, and emotions, we use regression, found best from experiments. We get results like anticipation, excitement, engagement, confidence, pleasure, happiness, and affection.",6,14.678461538461562
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression to recognize balance, land dominance, and emotions, shown best by experiments. Results include recognizing anticipation, excitement, engagement, confidence, pleasure, happiness, and affection.",7,1.2595000000000312
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression for recognizing balance, land dominance, and emotions using regression, best by our experiments. Our results include anticipation, excitement, engagement, confidence, happiness, and affection.",8,8.170769230769253
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression for recognizing balance, land dominance, and emotions. Experiments showed it's best. Results include anticipation, excitement, confidence, happiness, and affection in images.",9,8.365000000000009
"And we separate the recognition of balance, and also land dominance, and the emotion categories. And the type of loss function that we use is regression, because from our experiments, we saw that it was the best way to model our data. So these are the type of results that we get. So these are the type of images that we get. This is recognition of anticipation, excitement, engagement, and confidence in this case. This is another example of recognition, pleasure, happiness, and affection in this other picture.","We use regression to identify balance, land dominance, and emotions. The experiments showed this is best. Results include recognizing anticipation, excitement, confidence, and happiness in pictures.",1,15.822948717948748
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, indicating it uses context. Another example showed it can detect multiple emotions based on both context and the person. However, the system isn't very effective yet. This is the first attempt to recognize emotions using context and person. The project is developing further.",1,42.2114117647059
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system could identify happiness without seeing the face, using context. In another case, it detected various emotions, using context and the person. The system is not very reliable yet. This is a first attempt to use context and person to recognize emotions. The project continues.",2,50.366565217391326
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, implying it uses context. In another case, it identified various emotions, combining context and the person. However, the system is not very effective yet. This is its first attempt to use both context and person for emotion recognition, and the project is ongoing.",3,42.94080882352944
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, hinting it uses context. It identified various emotions using both context and the person. The system is not very effective yet. This is its first attempt to combine context and person for emotion recognition. The project is ongoing.",4,48.52743478260871
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, indicating it uses context. In another instance, it identified various emotions using context and the person, but it is not very effective yet. This is its first attempt combining context and person for emotion recognition and the project is developing.",5,33.732500000000016
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, so it uses context. Another case showed it identifies many emotions using both context and person. The system is not very effective yet. This is its first attempt at combining context and person for emotion recognition, and the project is developing.",6,45.91961734693879
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, hinting it uses context. Another case showed it can detect emotions by combining context and person. The system still needs improvement. This is its first attempt at using context and person for emotions, and the project continues.",7,54.41625000000002
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, indicating it uses context. Another case showed it can detect emotions by combining context and person. The system needs improvement. This is its first attempt at using context and person for emotions. The project continues.",8,50.54786046511629
"Another one pretty challenging that here is, it recognized happiness, the system, but here you don't see the face. So clearly somehow it's extracting some information about the context and the situation. Here another interesting example where for this guy, and the recognition of emotions was like pleasure, affection and happiness, but for the other person, it was like a mess, so it recognized almost everything possible, and so this is showing that not all the information is coming just from the context, but it's the combination of the context and the person which actually creates the output. So the truth is that this system is not working very well, so this is the first attempt in trying to recognize emotions using not just face, but using person and situational context. So this is our first baseline. We are progressing this project in different directions.","The system recognized happiness without seeing the face, so it uses context. Another case showed it can detect emotions by combining context and person. The system still needs work. This is its first attempt at using context and person for emotions, and the project continues.",9,60.056250000000034
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House restaurant is in Cambridge, Massachusetts. If you've never been and want to know more, you can search for reviews online. One review says it's delightful with a great menu, craft cocktails, and an unpretentious atmosphere frequented by locals and college professors.",1,55.434696969697
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge, MA, has great reviews. If you check online, one review mentions a delightful spot with a terrific menu, craft cocktails, and a relaxed vibe with locals and college professors.",2,62.68000000000002
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge, MA, is well-reviewed. An online review praises its menu, craft cocktails, and a casual setting with locals and college professors.",3,55.40350000000004
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge has good reviews. One review mentions a great menu, craft cocktails, and a friendly atmosphere with locals and professors.",4,67.75500000000001
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge, MA, is well-liked. One review highlights its menu, cocktails, and a friendly setting frequented by locals and professors.",5,62.74510869565219
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge has a positive review. It praises the menu, cocktails, and a relaxed atmosphere with locals and professors.",6,57.23363636363638
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House in Cambridge gets good reviews. One says it has a great menu, cocktails, and a friendly atmosphere with locals and professors.",7,74.805
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House has good reviews. One praises the menu, cocktails, and the friendly atmosphere.",8,75.14250000000001
"So the Red House restaurant is a restaurant in Cambridge, Massachusetts in USA. So if you go to this restaurant and you never went to this restaurant and you want to know how this restaurant works, so you Google this restaurant and you get all of these reviews. So this is one example of a review of the Red House restaurant. Okay, so let's focus in a piece of this review. review. So here it says, what's a delight, terrific menu, great craft, cocktails, unpretentious atmosphere of mostly locals and college professors chatting over dinner.","The Red House has great reviews. It is praised for its menu, cocktails, and friendly vibe.",9,87.67750000000001
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|",We read reviews to decide if we want to go to a restaurant. They don't say directly if it's good or bad. Can you tell if the review is positive? It's clear.|,9,82.39000000000001
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|",We read reviews to decide on restaurants. They don't say outright if it's good or bad. Can you tell if the text is positive? It's clear.|,8,83.09903846153848
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|",Reviews help decide on visiting restaurants. They don't specify if it's good or bad. Can you tell if the review is positive or negative? It's evident.|,7,57.06826923076926
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","We read reviews to choose restaurants. They don't state clearly if it's good or bad. From this, can you tell if the review is positive or negative? It is clear.|",6,83.6025
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","Reviews help us decide on restaurants, but they don't clearly say if it's good or bad. Can you deduce if it's positive or negative from this? It's clear.|",5,70.46166666666669
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","We read different reviews to decide, though they don’t explicitly say if the restaurant is good or bad. Can you perceive if it's positive or negative? It's evident.|",4,58.37595238095241
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","Although reviews help us decide if we should visit a restaurant, they don’t plainly say if it's good or bad. Can you infer the sentiment from this text? It seems clear.|",3,81.727311827957
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","Reading reviews guide us to decide on visiting restaurants, though they don’t declare whether it's good or bad. Can this text reveal if it's positive or negative? It is clear.|",2,66.965
"Okay, so we read this type of reviews and from these reviews, we decide whether we want to go to the restaurant or not. Somehow we capture the idea of whether this restaurant is good or not. But it is not saying, this is a good restaurant, go there or this is not a good restaurant, don't go there. So from this type of information, can you tell me if the person that wrote this text was feeling something positive or something negative about the restaurant? Is it positive? It's very clear.|","Reviews are read to decide on restaurant visits, but they don't explicitly state if the restaurant is good or bad. Can the text help determine if it is positive or negative? It appears clear.|",1,65.94343137254904
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|",We find it easy. We think of algorithms doing the same. Research tries to capture text sentiment. This is called sentiment analysis. One model is deep moji.|,9,69.75400000000002
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","It's easy for us. We might create algorithms to do the same. There's much research on capturing sentiment from text. This is called sentiment analysis, and deep moji is a top model.|",8,66.5275
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|",It's easy for us. We might design similar algorithms. There's much research to capture sentiment in text. This field is called sentiment analysis. One top model is deep moji.|,7,60.92041379310348
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","It's easy for us to do. We might develop algorithms to do the same. Researchers work on capturing sentiment from text. This is known as sentiment analysis, and deep moji is a leading model.|",6,63.84279411764709
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","It is very easy for us to do. Perhaps, we might develop algorithms that can do the same. Actually, there is extensive research in capturing text sentiment. This task is termed sentiment analysis in text. One state of the art model is named deep moji.|",5,66.10000000000002
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","It's fairly easy for us to do. We might consider developing algorithms to perform the same tasks. Research in this area often involves capturing sentiment from text, a process known as sentiment analysis. One highly regarded model in this domain is called deep moji.|",4,47.62000000000003
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","We find it fairly straightforward to perform this task. Consequently, we may consider developing algorithms that can achieve similar results. In fact, extensive research has been conducted with the goal of extracting sentiment from textual data, a task referred to as sentiment analysis. Among the notable models in this area, one is referred to as deep moji.|",3,43.95019736842107
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","For us, accomplishing this task is relatively simple. Consequently, it is conceivable that we could create algorithms to perform equivalent functions. Furthermore, there is a substantial amount of research focusing on the extraction of sentiment from text, a field categorized under sentiment analysis in textual data. One exemplary model developed in this area is known as deep moji.|",2,28.7519827586207
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","Performing this task is relatively effortless for us, which leads to the possibility of creating algorithms capable of yielding comparable outcomes. More importantly, a significant body of research is dedicated to the extraction of sentiment from textual content, an endeavor categorized under sentiment analysis in textual data. In this domain, a notable state-of-the-art model is referred to as deep moji.|",1,14.515000000000043
"For us, it's very easy to do. So one thing that we can think is maybe we can develop algorithms that do the same. So actually, there's a lot of research in trying to capture sentiment from text. This problem is called sentiment analysis in text. And this is one of the state of the art models. It's called deep moji.|","Performing the task is quite easy for us, which suggests the possibility of designing algorithms capable of producing comparable results. Additionally, a significant volume of research focuses on the extraction of sentiment from textual inputs, a field generally referred to as sentiment analysis in text. A prominent state-of-the-art model in this field is known as deep moji.|",1,24.28684210526319
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","The MIT Media Lab, not the effective computing group I work with, developed this simple method using social machines. They used deep learning on a large dataset of 1,200 million tweets with 64 common emojis.",1,63.381071428571445
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","The MIT Media Lab, not my effective computing group, devised a method using social machines and deep learning with 1,200 million emoji tweets. They chose the 64 most common emojis.",2,61.890000000000015
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","MIT Media Lab created this method, not the group I work with. The method is simple, using social machines and deep learning on 1,200 million tweets with 64 common emojis.",3,73.17000000000003
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","Created by MIT Media Lab, not my group, this method uses social machines and deep learning with 1,200 million tweets featuring 64 common emojis.",4,52.05000000000001
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","MIT Media Lab made it, not my group. The method, using social machines and deep learning, analyzed 1,200 million tweets with 64 common emojis.",5,71.28000000000002
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","MIT Media Lab created this, not my group. This method is simple, using social machines and deep learning on 1,200 million tweets with 64 common emojis.",6,73.24769230769233
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","MIT Media Lab, not my group, developed this. The simple method uses social machines and large-scale data, analyzing 1,200 million tweets with 64 common emojis.",7,55.40350000000004
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","The MIT Media Lab created it, not my group. This simple method employs social machines and deep learning with 1,200 million tweets having 64 common emojis.",8,69.99384615384618
"It was developed in MIT media lab, not in effective computing group, which is the group I'm working on. It was developed on social machines. And the idea behind this method is quite simple. So there's large-scale data behind this and deep learning. So basically, they took 1,200 million tweets containing emojis. So they selected the most common, the 64 most common emojis, and they were collecting a huge amount of tweets that contained these emojis.","MIT Media Lab, not my group, developed it. The simple method uses social machines and deep learning on 1,200 million tweets with 64 common emojis.",9,68.93950000000002
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","The problem proposed is to predict emojis from the text using a deep learning model, with code and an online demo available to see the output.",1,53.54500000000003
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","They created a task to predict emojis from text, with available deep learning code and an online demo.",2,52.265000000000015
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","They set a problem to predict emojis using text, with available code and an online demo to show results.",3,58.42368421052632
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","They aimed to predict emojis from text via deep learning, with a demo and code available online.",4,50.2388235294118
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.",They proposed predicting emojis from text with available deep learning code and an online demo.,5,44.97000000000003
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","They want to predict emojis from text using deep learning, with a demo and code available.",6,63.69500000000001
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.",Predict emojis from text using deep learning; code and demo online.,7,64.9245454545455
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.",Predict emojis from text with a deep learning model; demo online.,8,57.23363636363638
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.","Enter text, see predicted emojis with deep learning.",9,50.66500000000002
"And they formulated the following problem. So taking as an input the text, we want to predict the emoji of this text. And when, so they have a deep learning architecture, you can actually, the code is available and they have also a demo online. So this is a screen capture of the demo. So if you enter the text I showed you before, you'll see this type of output. So this is the emojis that are predicted from the text.",Enter text to get emoji predictions.,1,45.64500000000001
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",Viewing varying word intensities here is fascinating since an attention layer in the model helps gauge word contributions to predictions. The demo is intriguing as it showcases how changing word placements alters emoji outcomes. This model and others perform well in tech sentiment analysis.,1,36.20742424242428
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|","Here, you observe different word intensities due to an attention layer in the deep learning model, understanding word influence on predictions. The demo is engaging, showing emoji changes with words removed. Other models also excel in tech sentiment analysis.",2,33.116923076923115
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The different word intensities seen here are due to the deep learning model’s attention layer. This demo is cool as it shows how emojis change when words are removed. Similar models also work well for tech sentiment analysis.,3,58.173070175438625
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|","The attention layer in this deep learning model reveals varied word intensities, impacting predictions. This demo impressively shows how removing words changes emojis. Similar models are effective in tech sentiment analysis.",4,18.959569892473127
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The deep learning model's attention layer shows word intensity variations in predictions. This demo shows changing emojis when words are altered. Similar models perform well in tech sentiment analysis.,5,33.65781609195403
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The deep learning model's attention layer shows different word intensities. The demo is fun as removing words changes emojis. Other models work well for tech sentiment analysis too.,6,40.24738095238098
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The model's attention layer shows varied word intensities. Removing words changes emojis in the demo. Other models also do well in tech sentiment analysis.,7,33.04000000000002
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The model's attention layer reveals word intensity. The demo shows emoji changes with word removal. Models like this are good for tech sentiment analysis.,8,40.09
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The attention layer shows word intensity. The demo changes emojis with word removal. Models like this excel in sentiment analysis.,9,35.09833333333333
"And what is interesting is that you see here different intensities of the words. This is because the deep learning model has an attention layer. So it can capture the contribution of the different words to this prediction. And actually, this demo is very cool, because you can cross the different words and see how the emojis change when you remove some words. So this is working pretty well. And there are other models that are similar to this one that also work pretty well for tech sentiment analysis.|",The model shows word intensity. Emojis change in the demo with removed words. Similar models are good for analysis.,1,53.4698245614035
